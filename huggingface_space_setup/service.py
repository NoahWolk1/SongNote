#!/usr/bin/env python3"""Neural Singing Voice Synthesis ServiceUses neural vocoders and singing-specific models for realistic singing synthesis"""import osimport base64import tempfileimport jsonimport mathimport randomfrom flask import Flask, request, jsonifyfrom flask_cors import CORSimport numpy as npimport librosaimport soundfile as sffrom scipy import signalimport torchimport torchaudioapp = Flask(__name__)CORS(app)# Global model variablessinging_model = Noneclass NeuralSingingVoice:    """Neural singing voice synthesizer using advanced vocoder techniques"""        def __init__(self):        self.sample_rate = 22050        self.hop_length = 256        self.n_fft = 1024        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'            def text_to_phonemes(self, text):        """Convert text to phonemes for singing synthesis"""        # Enhanced phoneme conversion for singing        phonemes = []        words = text.lower().split()                # Vowel elongation mapping for singing        vowel_map = {            'a': ['aa', 'ah'], 'e': ['eh', 'ey'], 'i': ['ih', 'iy'],             'o': ['oh', 'ow'], 'u': ['uh', 'uw']        }                for word in words:            word_phonemes = []            for char in word:                if char in vowel_map:                    # Elongate vowels for singing                    word_phonemes.extend(vowel_map[char] * 2)                elif char.isalpha():                    word_phonemes.append(char)            phonemes.extend(word_phonemes)            phonemes.append('_')  # Word boundary                return phonemes        def generate_singing_f0(self, phonemes, voice_style='ballad', mood='happy'):        """Generate fundamental frequency (F0) contour for singing"""        duration = len(phonemes) * 0.15  # ~150ms per phoneme        time_steps = int(duration * self.sample_rate / self.hop_length)                # Base frequency ranges by voice style        f0_ranges = {            'ballad': (160, 350),    # Lower, more intimate            'pop': (180, 450),       # Mid-range, versatile            'rock': (200, 500),      # Higher, more powerful            'jazz': (140, 380),      # Lower, more sultry            'opera': (220, 600)      # Wide range, dramatic        }                # Mood adjustments        mood_multipliers = {            'happy': 1.1,      # Slightly higher pitch            'sad': 0.9,        # Lower pitch            'energetic': 1.15, # Higher and more varied            'romantic': 0.95,  # Slightly lower, more stable            'angry': 1.2       # Higher, more aggressive        }                base_range = f0_ranges.get(voice_style, f0_ranges['pop'])        mood_mult = mood_multipliers.get(mood, 1.0)                f0_min = base_range[0] * mood_mult        f0_max = base_range[1] * mood_mult                # Generate musical F0 contour        t = np.linspace(0, duration, time_steps)                # Create melodic patterns        base_f0 = f0_min + (f0_max - f0_min) * 0.6  # Start at 60% of range                # Add musical intervals and vibrato        melody_pattern = np.sin(2 * np.pi * t * 0.8) * 0.3  # Slow melody        vibrato = np.sin(2 * np.pi * t * 5.5) * 0.02        # Fast vibrato        tremolo = np.sin(2 * np.pi * t * 0.3) * 0.1         # Slow tremolo                # Combine all elements        f0_contour = base_f0 * (1 + melody_pattern + vibrato + tremolo)                # Add note transitions (legato style)        for i in range(1, len(f0_contour)):            if abs(f0_contour[i] - f0_contour[i-1]) > 50:  # Large jump                # Smooth the transition                steps = min(10, len(f0_contour) - i)                for j in range(steps):                    alpha = j / steps                    f0_contour[i+j] = f0_contour[i-1] * (1-alpha) + f0_contour[i+j] * alpha                return f0_contour        def synthesize_singing(self, text, voice_style='ballad', mood='happy'):        """Main singing synthesis function using neural vocoder approach"""        try:            # Convert text to phonemes            phonemes = self.text_to_phonemes(text)                        # Generate singing F0 (pitch) contour            f0_contour = self.generate_singing_f0(phonemes, voice_style, mood)                        # Create harmonic content for singing voice            duration = len(f0_contour) * self.hop_length / self.sample_rate            time = np.linspace(0, duration, int(duration * self.sample_rate))                        # Generate singing waveform using additive synthesis            audio = self.generate_singing_waveform(f0_contour, time, voice_style, mood)                        # Apply singing-specific effects            audio = self.apply_singing_effects(audio, voice_style, mood)                        return audio                    except Exception as e:            print(f"Error in singing synthesis: {e}")            # Fallback to simple synthesis            return self.generate_fallback_singing(text, voice_style, mood)        def generate_singing_waveform(self, f0_contour, time, voice_style, mood):        """Generate singing waveform using harmonic synthesis"""        audio = np.zeros_like(time)                # Harmonic content varies by voice style        harmonic_weights = {            'ballad': [1.0, 0.7, 0.4, 0.3, 0.2, 0.1],      # Warm, smooth            'pop': [1.0, 0.8, 0.6, 0.4, 0.3, 0.2],         # Bright, clear            'rock': [1.0, 0.9, 0.7, 0.6, 0.4, 0.3],        # Powerful, edgy            'jazz': [1.0, 0.6, 0.5, 0.4, 0.3, 0.2],        # Smooth, mellow            'opera': [1.0, 0.8, 0.7, 0.6, 0.5, 0.4]        # Rich, full        }                weights = harmonic_weights.get(voice_style, harmonic_weights['pop'])                # Generate harmonics        for harmonic, weight in enumerate(weights, 1):            if weight > 0.1:                # Interpolate F0 for each time point                f0_interpolated = np.interp(time,                                           np.linspace(0, time[-1], len(f0_contour)),                                           f0_contour)                                # Generate harmonic with frequency modulation                harmonic_freq = f0_interpolated * harmonic                phase = 2 * np.pi * np.cumsum(harmonic_freq) / self.sample_rate                                # Add some natural variation                noise = np.random.normal(0, 0.01, len(phase))                harmonic_wave = weight * np.sin(phase + noise)                                # Apply envelope to make it more voice-like                envelope = self.generate_vocal_envelope(time, voice_style)                harmonic_wave *= envelope                                audio += harmonic_wave                # Add formant characteristics for human voice        audio = self.apply_formants(audio, voice_style)                # Normalize        if np.max(np.abs(audio)) > 0:            audio = audio / np.max(np.abs(audio)) * 0.8                return audio        def generate_vocal_envelope(self, time, voice_style):        """Generate envelope that mimics vocal characteristics"""        envelope = np.ones_like(time)                # Attack and decay patterns        attack_time = 0.05  # 50ms attack        decay_time = 0.1    # 100ms decay                # Style-specific envelope shapes        if voice_style == 'ballad':            # Smooth, sustained envelope            for i, t in enumerate(time):                segment = t % 1.0  # 1-second segments                if segment < attack_time:                    envelope[i] = segment / attack_time                elif segment > 0.8:  # Gentle decay at end                    envelope[i] = 1.0 - (segment - 0.8) / 0.2                elif voice_style == 'rock':            # More aggressive, punchy envelope            for i, t in enumerate(time):                segment = t % 0.8  # Shorter segments                if segment < attack_time:                    envelope[i] = (segment / attack_time) ** 0.5  # Faster attack                # Add breathing and natural variations        breath_pattern = 0.05 * np.sin(2 * np.pi * time * 0.3)  # Slow breathing        envelope += breath_pattern                return np.clip(envelope, 0.1, 1.0)        def apply_formants(self, audio, voice_style):        """Apply formant filtering to make audio sound more human-like"""        # Formant frequencies for different vowels (simplified)        formant_freqs = {            'ballad': [(800, 1200, 2500), (400, 2000, 2800)],    # Warm vowels            'pop': [(700, 1220, 2600), (500, 1900, 2700)],       # Bright vowels            'rock': [(600, 1100, 2400), (450, 1800, 2600)],      # Powerful vowels            'jazz': [(750, 1300, 2700), (400, 2100, 2900)],      # Smooth vowels            'opera': [(650, 1080, 2650), (400, 1600, 2700)]      # Classical vowels        }                freqs = formant_freqs.get(voice_style, formant_freqs['pop'])                # Apply formant filtering        filtered_audio = audio.copy()        for f1, f2, f3 in freqs:            # Create bandpass filters for each formant            for freq in [f1, f2, f3]:                sos = signal.butter(4, [freq-100, freq+100], btype='band',                                   fs=self.sample_rate, output='sos')                formant_component = signal.sosfilt(sos, audio) * 0.3                filtered_audio += formant_component                return filtered_audio        def apply_singing_effects(self, audio, voice_style, mood):        """Apply singing-specific audio effects"""        # Add reverb for singing space        reverb_audio = self.add_singing_reverb(audio, voice_style)                # Add chorus effect for richness        chorus_audio = self.add_chorus_effect(audio)                # Combine effects        final_audio = 0.7 * reverb_audio + 0.3 * chorus_audio                # Dynamic compression for singing        final_audio = self.apply_vocal_compression(final_audio)                # EQ adjustments by style        final_audio = self.apply_singing_eq(final_audio, voice_style)                return final_audio        def add_singing_reverb(self, audio, voice_style):        """Add appropriate reverb for singing voice"""        # Different reverb characteristics by style        reverb_params = {            'ballad': {'delay': 0.1, 'decay': 0.6, 'mix': 0.3},            'pop': {'delay': 0.05, 'decay': 0.4, 'mix': 0.2},            'rock': {'delay': 0.08, 'decay': 0.5, 'mix': 0.25},            'jazz': {'delay': 0.12, 'decay': 0.7, 'mix': 0.35},            'opera': {'delay': 0.15, 'decay': 0.8, 'mix': 0.4}        }                params = reverb_params.get(voice_style, reverb_params['pop'])                # Simple reverb implementation        delay_samples = int(params['delay'] * self.sample_rate)        decay_factor = params['decay']        mix = params['mix']                reverb = np.zeros_like(audio)        for i in range(delay_samples, len(audio)):            reverb[i] = audio[i] + decay_factor * reverb[i - delay_samples]                return (1 - mix) * audio + mix * reverb        def add_chorus_effect(self, audio):        """Add chorus effect for vocal richness"""        # Multiple delayed copies with slight pitch variations        chorus = audio.copy()                delays = [0.02, 0.035, 0.05]  # Multiple delay times        for delay_time in delays:            delay_samples = int(delay_time * self.sample_rate)            delayed = np.roll(audio, delay_samples) * 0.3            chorus += delayed                return chorus / len(delays)        def apply_vocal_compression(self, audio):        """Apply dynamic compression suitable for vocals"""        # Simple soft compression        threshold = 0.5        ratio = 3.0                compressed = audio.copy()        for i in range(len(audio)):            if abs(audio[i]) > threshold:                excess = abs(audio[i]) - threshold                compressed_excess = excess / ratio                compressed[i] = np.sign(audio[i]) * (threshold + compressed_excess)                return compressed        def apply_singing_eq(self, audio, voice_style):        """Apply EQ adjustments for different singing styles"""        # Style-specific EQ curves        if voice_style == 'ballad':            # Boost low-mids, gentle high-end            audio = self.apply_eq_band(audio, 200, 400, 1.2)    # Warmth            audio = self.apply_eq_band(audio, 2000, 4000, 1.1)  # Presence                elif voice_style == 'pop':            # Bright, clear sound            audio = self.apply_eq_band(audio, 1000, 3000, 1.3)  # Clarity            audio = self.apply_eq_band(audio, 5000, 8000, 1.2)  # Air                elif voice_style == 'rock':            # Powerful, cutting sound            audio = self.apply_eq_band(audio, 800, 1600, 1.4)   # Power            audio = self.apply_eq_band(audio, 3000, 5000, 1.2)  # Cut                return audio        def apply_eq_band(self, audio, low_freq, high_freq, gain):        """Apply EQ boost/cut to a frequency band"""        sos = signal.butter(4, [low_freq, high_freq], btype='band',                           fs=self.sample_rate, output='sos')        band_audio = signal.sosfilt(sos, audio)        return audio + (gain - 1.0) * band_audio        def generate_fallback_singing(self, text, voice_style, mood):        """Fallback simple singing synthesis"""        duration = len(text) * 0.15  # Rough duration estimate        time = np.linspace(0, duration, int(duration * self.sample_rate))                # Simple melody        base_freq = 220  # A3        melody = base_freq * (1 + 0.3 * np.sin(2 * np.pi * time * 0.5))                # Generate simple singing waveform        audio = np.sin(2 * np.pi * melody * time)                # Add vibrato        vibrato = 1 + 0.05 * np.sin(2 * np.pi * time * 6)        audio *= vibrato                # Apply envelope        envelope = np.exp(-time * 0.5)  # Decay envelope        audio *= envelope                return audiodef load_singing_model():    """Initialize the neural singing voice synthesizer"""    global singing_model    try:        singing_model = NeuralSingingVoice()        print("Loaded Neural Singing Voice Synthesizer successfully")    except Exception as e:        print(f"Error loading singing model: {e}")        singing_model = None@app.route('/health', methods=['GET'])def health_check():    return jsonify({        "status": "healthy",         "service": "neural-singing-tts",        "model_loaded": singing_model is not None    })@app.route('/generate-singing', methods=['POST'])def generate_singing():    try:        data = request.get_json()        lyrics = data.get('lyrics', '')        voice_style = data.get('voice_style', 'ballad')        mood = data.get('mood', 'happy')                if not lyrics:            return jsonify({"error": "No lyrics provided"}), 400                if singing_model is None:            return jsonify({"error": "Singing model not loaded"}), 500                # Create temporary output file        output_path = tempfile.mktemp(suffix='.wav')                try:            # Generate singing using neural synthesis            audio = singing_model.synthesize_singing(lyrics, voice_style, mood)                        # Save audio file            sf.write(output_path, audio, singing_model.sample_rate)                        # Read the generated audio file            with open(output_path, 'rb') as audio_file:                audio_data = audio_file.read()                        # Convert to base64 for transport            audio_base64 = base64.b64encode(audio_data).decode('utf-8')            audio_url = f"data:audio/wav;base64,{audio_base64}"                        return jsonify({                "audio_url": audio_url,                "format": "wav",                "duration_seconds": len(audio) / singing_model.sample_rate,                "synthesis_method": "neural_singing_voice",                "voice_style": voice_style,                "mood": mood            })                    finally:            # Clean up temporary file            if os.path.exists(output_path):                os.unlink(output_path)                except Exception as e:        print(f"Error generating singing: {str(e)}")        return jsonify({"error": f"Failed to generate singing: {str(e)}"}), 500@app.route('/test-singing', methods=['GET'])def test_singing():    """Quick test endpoint for singing synthesis"""    try:        if singing_model is None:            return jsonify({"error": "Singing model not loaded"}), 500                # Generate a quick test        test_lyrics = "La la la la la"        audio = singing_model.synthesize_singing(test_lyrics, 'pop', 'happy')                return jsonify({            "status": "success",            "test_lyrics": test_lyrics,            "audio_duration": len(audio) / singing_model.sample_rate,            "synthesis_method": "neural_singing_voice"        })            except Exception as e:        return jsonify({"error": f"Test failed: {str(e)}"}), 500if __name__ == '__main__':    print("🎵 Starting Neural Singing Voice Synthesis Service...")    load_singing_model()    print("🎤 Service ready on port 8002")    app.run(host='0.0.0.0', port=8002, debug=False)